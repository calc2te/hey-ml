{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "1 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "2 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "3 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "4 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "5 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "6 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "7 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "8 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "9 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "10 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "11 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "12 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "13 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "14 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "15 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "16 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "17 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "18 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "19 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "20 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "21 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "22 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "23 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "24 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "25 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "26 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "27 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "28 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "29 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "30 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "31 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "32 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "33 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "34 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "35 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "36 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "37 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "38 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "39 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "40 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "41 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "42 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "43 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "44 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "45 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "46 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "47 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "48 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "49 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "50 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "51 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "52 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "53 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "54 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "55 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "56 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "57 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "58 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "59 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "60 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "61 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "62 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "63 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "64 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "65 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "66 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "67 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "68 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "69 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "70 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "71 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "72 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "73 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "74 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "75 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "76 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "77 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "78 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "79 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "80 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "81 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "82 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "83 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "84 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "85 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "86 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "87 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "88 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "89 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "90 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "91 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "92 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "93 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "94 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "95 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "96 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "97 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "98 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "99 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "100 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "101 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "102 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "103 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "104 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "105 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "106 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "107 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "108 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "109 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "110 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "111 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "112 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "113 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "114 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "115 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "116 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "117 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "118 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "119 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "120 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "121 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "122 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "123 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "124 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "125 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "126 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "127 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "128 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "129 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "130 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "131 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "132 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "133 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "134 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "135 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "136 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "137 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "138 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "139 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "140 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "141 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "142 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "143 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "144 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "145 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "146 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "147 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "148 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "149 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "150 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "151 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "152 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "153 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "154 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "155 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "156 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "157 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "158 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "159 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "160 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "161 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "162 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "163 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "164 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "165 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "166 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "167 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "168 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "169 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "170 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "171 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "172 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "173 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "174 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "175 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "176 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "177 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "178 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "179 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "180 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "181 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "182 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "183 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "184 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "185 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "186 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "187 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "188 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "189 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "190 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "191 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "192 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "193 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "194 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "195 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "196 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "197 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "198 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "199 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "200 5.73203 [[ 0.80269563  0.67861295 -1.2172831 ]\n",
      " [-0.3051686  -0.3032113   1.508257  ]\n",
      " [ 0.7572236  -0.7008909  -2.108204  ]]\n",
      "Prediction: [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.73203 [[-0.30548954  1.2298503  -0.66033536]\n",
      " [-4.3907      2.2967086   2.9938684 ]\n",
      " [-3.345107    2.0974321  -0.80419564]]\n",
      "20 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "40 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "60 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "80 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "100 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "120 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "140 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "160 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "180 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "200 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "Prediction: [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if (step % 20 == 0):\n",
    "            print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.73203 [[ 0.7288166   0.7153621  -1.1801533 ]\n",
      " [-0.57753736 -0.12988332  1.6072978 ]\n",
      " [ 0.48373488 -0.51433605 -2.02127   ]]\n",
      "20 1.6610024 [[ 0.35837555  0.7006911  -0.79504126]\n",
      " [-0.5218104  -0.14752226  1.5692096 ]\n",
      " [-0.00524238 -0.21694626 -1.8296826 ]]\n",
      "40 1.4070582 [[ 0.08632906  0.6441534  -0.4664571 ]\n",
      " [-0.27831453 -0.19541714  1.3736086 ]\n",
      " [-0.14105876 -0.16434807 -1.7464645 ]]\n",
      "60 1.2188071 [[-0.14574495  0.580433   -0.17066269]\n",
      " [-0.1107576  -0.17286882  1.1835034 ]\n",
      " [-0.2245029  -0.17601469 -1.6513535 ]]\n",
      "80 1.0710086 [[-0.3455095   0.5168773   0.09265763]\n",
      " [ 0.01382132 -0.11677328  1.0028291 ]\n",
      " [-0.28096494 -0.21812555 -1.5527805 ]]\n",
      "100 0.9537603 [[-0.52009284  0.45806164  0.32605675]\n",
      " [ 0.11236367 -0.04872466  0.836238  ]\n",
      " [-0.3214388  -0.27116382 -1.4592681 ]]\n",
      "120 0.86215675 [[-0.67484623  0.40643024  0.5324416 ]\n",
      " [ 0.19237746  0.01887316  0.68862635]\n",
      " [-0.3494764  -0.32348    -1.3789142 ]]\n",
      "140 0.7924157 [[-0.81354636  0.36310616  0.714466  ]\n",
      " [ 0.25704384  0.07866935  0.5641637 ]\n",
      " [-0.3659038  -0.3680878  -1.3178791 ]]\n",
      "160 0.7402977 [[-0.9389806   0.32821035  0.874796  ]\n",
      " [ 0.30803508  0.12709346  0.4647484 ]\n",
      " [-0.37131408 -0.40190816 -1.2786481 ]]\n",
      "180 0.7011599 [[-1.0534288   0.30107263  1.016382  ]\n",
      " [ 0.34702814  0.16383716  0.38901168]\n",
      " [-0.3671113  -0.4252324  -1.2595268 ]]\n",
      "200 0.67090875 [[-1.1588541   0.28058422  1.1422957 ]\n",
      " [ 0.37609792  0.19073224  0.33304682]\n",
      " [-0.35536593 -0.44033223 -1.2561723 ]]\n",
      "Prediction: [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run(\n",
    "            [cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if (step % 20 == 0):\n",
    "            print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "xz\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  2019921800000.0 \n",
      "Prediction:\n",
      " [[1003463.9]\n",
      " [2018910.5]\n",
      " [1588441.4]\n",
      " [1113822. ]\n",
      " [1312501. ]\n",
      " [1323536.8]\n",
      " [1213151.5]\n",
      " [1544276.8]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if (step % 20 == 0):\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.28659973e+02, 8.33450012e+02, 9.08100000e+05, 8.28349976e+02,\n",
       "        8.31659973e+02],\n",
       "       [8.23020020e+02, 8.28070007e+02, 1.82810000e+06, 8.21655029e+02,\n",
       "        8.28070007e+02],\n",
       "       [8.19929993e+02, 8.24400024e+02, 1.43810000e+06, 8.18979980e+02,\n",
       "        8.24159973e+02],\n",
       "       [8.16000000e+02, 8.20958984e+02, 1.00810000e+06, 8.15489990e+02,\n",
       "        8.19239990e+02],\n",
       "       [8.19359985e+02, 8.23000000e+02, 1.18810000e+06, 8.18469971e+02,\n",
       "        8.18979980e+02],\n",
       "       [8.19000000e+02, 8.23000000e+02, 1.19810000e+06, 8.16000000e+02,\n",
       "        8.20450012e+02],\n",
       "       [8.11700012e+02, 8.15250000e+02, 1.09810000e+06, 8.09780029e+02,\n",
       "        8.13669983e+02],\n",
       "       [8.09510010e+02, 8.16659973e+02, 1.39810000e+06, 8.04539978e+02,\n",
       "        8.09559998e+02]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = MinMaxScaler(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999999, 0.99999999, 0.        , 1.        , 1.        ],\n",
       "       [0.70548491, 0.70439552, 1.        , 0.71881782, 0.83755791],\n",
       "       [0.54412549, 0.50274824, 0.57608696, 0.606468  , 0.6606331 ],\n",
       "       [0.33890353, 0.31368023, 0.10869565, 0.45989134, 0.43800918],\n",
       "       [0.51436   , 0.42582389, 0.30434783, 0.58504805, 0.42624401],\n",
       "       [0.49556179, 0.42582389, 0.31521739, 0.48131134, 0.49276137],\n",
       "       [0.11436064, 0.        , 0.20652174, 0.22007776, 0.18597238],\n",
       "       [0.        , 0.07747099, 0.5326087 , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  0.10567288 \n",
      "Prediction:\n",
      " [[ 0.8542941 ]\n",
      " [ 1.562238  ]\n",
      " [ 0.86688405]\n",
      " [ 0.13131964]\n",
      " [ 0.48175365]\n",
      " [ 0.42280787]\n",
      " [-0.17695826]\n",
      " [ 0.150666  ]]\n",
      "20 Cost:  0.10565975 \n",
      "Prediction:\n",
      " [[ 0.85423154]\n",
      " [ 1.5621493 ]\n",
      " [ 0.86682063]\n",
      " [ 0.13128406]\n",
      " [ 0.48170334]\n",
      " [ 0.422759  ]\n",
      " [-0.17698473]\n",
      " [ 0.15062982]]\n",
      "40 Cost:  0.10564661 \n",
      "Prediction:\n",
      " [[ 0.85416895]\n",
      " [ 1.5620606 ]\n",
      " [ 0.866757  ]\n",
      " [ 0.13124853]\n",
      " [ 0.48165315]\n",
      " [ 0.42271012]\n",
      " [-0.17701119]\n",
      " [ 0.15059364]]\n",
      "60 Cost:  0.10563348 \n",
      "Prediction:\n",
      " [[ 0.85410637]\n",
      " [ 1.5619719 ]\n",
      " [ 0.8666937 ]\n",
      " [ 0.13121307]\n",
      " [ 0.48160297]\n",
      " [ 0.42266124]\n",
      " [-0.17703766]\n",
      " [ 0.15055746]]\n",
      "80 Cost:  0.10562041 \n",
      "Prediction:\n",
      " [[ 0.8540442 ]\n",
      " [ 1.5618837 ]\n",
      " [ 0.86663055]\n",
      " [ 0.1311779 ]\n",
      " [ 0.4815532 ]\n",
      " [ 0.4226128 ]\n",
      " [-0.17706373]\n",
      " [ 0.1505217 ]]\n",
      "100 Cost:  0.10560733 \n",
      "Prediction:\n",
      " [[ 0.8539828 ]\n",
      " [ 1.561796  ]\n",
      " [ 0.8665683 ]\n",
      " [ 0.13114357]\n",
      " [ 0.48150408]\n",
      " [ 0.4225651 ]\n",
      " [-0.17708904]\n",
      " [ 0.15048671]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if (step % 20 == 0):\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  0.10567288 \n",
      "Prediction:\n",
      " [[ 0.8542941 ]\n",
      " [ 1.562238  ]\n",
      " [ 0.86688405]\n",
      " [ 0.13131964]\n",
      " [ 0.48175365]\n",
      " [ 0.42280787]\n",
      " [-0.17695826]\n",
      " [ 0.150666  ]]\n",
      "20 Cost:  0.057819232 \n",
      "Prediction:\n",
      " [[ 0.83765113]\n",
      " [ 1.3057548 ]\n",
      " [ 0.74582505]\n",
      " [ 0.1585238 ]\n",
      " [ 0.44098973]\n",
      " [ 0.38048768]\n",
      " [-0.14570546]\n",
      " [ 0.09373164]]\n",
      "40 Cost:  0.033911753 \n",
      "Prediction:\n",
      " [[ 0.88954353]\n",
      " [ 1.1817288 ]\n",
      " [ 0.7119745 ]\n",
      " [ 0.22298026]\n",
      " [ 0.46213222]\n",
      " [ 0.39893907]\n",
      " [-0.08579984]\n",
      " [ 0.08840626]]\n",
      "60 Cost:  0.020581514 \n",
      "Prediction:\n",
      " [[ 0.9257903 ]\n",
      " [ 1.0892644 ]\n",
      " [ 0.68696475]\n",
      " [ 0.27144912]\n",
      " [ 0.4780322 ]\n",
      " [ 0.41287276]\n",
      " [-0.0394471 ]\n",
      " [ 0.08645877]]\n",
      "80 Cost:  0.013135838 \n",
      "Prediction:\n",
      " [[ 0.95152843]\n",
      " [ 1.0202949 ]\n",
      " [ 0.6684524 ]\n",
      " [ 0.30783197]\n",
      " [ 0.49000922]\n",
      " [ 0.42334393]\n",
      " [-0.00394234]\n",
      " [ 0.08593473]]\n",
      "100 Cost:  0.008970037 \n",
      "Prediction:\n",
      " [[0.97002053]\n",
      " [0.968838  ]\n",
      " [0.6547408 ]\n",
      " [0.33511448]\n",
      " [0.4990536 ]\n",
      " [0.43118578]\n",
      " [0.02308993]\n",
      " [0.08591594]]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if (step % 20 == 0):\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999999, 0.99999999, 0.        , 1.        ],\n",
       "       [0.70548491, 0.70439552, 1.        , 0.71881782],\n",
       "       [0.54412549, 0.50274824, 0.57608696, 0.606468  ],\n",
       "       [0.33890353, 0.31368023, 0.10869565, 0.45989134],\n",
       "       [0.51436   , 0.42582389, 0.30434783, 0.58504805],\n",
       "       [0.49556179, 0.42582389, 0.31521739, 0.48131134],\n",
       "       [0.11436064, 0.        , 0.20652174, 0.22007776],\n",
       "       [0.        , 0.07747099, 0.5326087 , 0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        ],\n",
       "       [0.83755791],\n",
       "       [0.6606331 ],\n",
       "       [0.43800918],\n",
       "       [0.42624401],\n",
       "       [0.49276137],\n",
       "       [0.18597238],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
